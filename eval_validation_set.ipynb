{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the skeleton of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SimpleCNN class that inherits pytorch.nn.module and implement a vanilla CNN\n",
    "\n",
    "# file vanilla_cnn_2.py\n",
    "# author: Kazi Mahbub Mutakabbir\n",
    "# Email:kazi.mahbub@tum.de\n",
    "# date 17-10-2019\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from progress.bar import Bar\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "\n",
    "    # batch shape for input: (3, 32, 32), 3 = RGB channels\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # super().__init__()\n",
    "\n",
    "        ### first convolutional layer ###\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
    "        # batch normalization\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(16)\n",
    "        # reLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        # pooling layerZ\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # kernel size after 1st conv layer: 111*111*16\n",
    "\n",
    "        ### 2nd convolutional layer ###\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "        # batch normalization layer\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(32)\n",
    "        # pooling layer\n",
    "        # 1 padding added to get whole value of kernel dimension\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        # kernel size after 2nd conv layer: 55*55*32\n",
    "\n",
    "        ### 3rd convolutional layer ###\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "        # batch normalization layer\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(64)\n",
    "        # pooling layer\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # kernel size after 3rd conv layer: 26*26*64\n",
    "\n",
    "        ### 4th convolutional layer ###\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0)\n",
    "        # batch normalization layer\n",
    "        self.batch_norm_4 = nn.BatchNorm2d(128)\n",
    "        # pooling layer\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # kernel size after 4th convolutional layer: 12*12*128\n",
    "\n",
    "        ### 5th convolutional layer ###\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=0)\n",
    "        # batch normalization layer\n",
    "        self.batch_norm_5 = nn.BatchNorm2d(256)\n",
    "        # pooling layer\n",
    "        self.maxpool5 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # kernel size after 5th convolutional layer: 5*5*256\n",
    "\n",
    "        # fully connected layer 1\n",
    "        self.fc1 = nn.Linear(in_features=256 * 5 * 5, out_features=4096)\n",
    "        self.batch_norm_6 = nn.BatchNorm1d(4096)\n",
    "        # add a dropout to reduce overfitting\n",
    "        #self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        # fully connected layer 2\n",
    "        self.fc2 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        # add a dropout to reduce overfitting\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        # fully connected layer 3\n",
    "        self.fc3 = nn.Linear(1024, 256)\n",
    "        # add a dropout\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # output layer\n",
    "        self.fc4 = nn.Linear(256,2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute the activation of the first convolutional layer\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        # compute the activation of the second convolutional layer\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool2(out)\n",
    "        \n",
    "        # compute the activation of the third convolutional layer\n",
    "        out = self.conv3(out)\n",
    "        out = self.batch_norm_3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool3(out)\n",
    "\n",
    "        # compute the activation of the fourth convolutional layer\n",
    "        out = self.conv4(out)\n",
    "        out = self.batch_norm_4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool4(out)\n",
    "\n",
    "        # compute the activation of the fourth convolutional layer\n",
    "        out = self.conv5(out)\n",
    "        out = self.batch_norm_5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool5(out)\n",
    "        # print(\"Shape after conv: \" + str(out.shape))\n",
    "        # Reshape data for the input layer of the net\n",
    "        # Recall that the -1 infers this dimension\n",
    "\n",
    "        out = out.view(-1, 256 * 5 * 5)\n",
    "\n",
    "        # last conv layer -> 1st fc layer\n",
    "        out = self.fc1(out)\n",
    "        out = self.batch_norm_6(out)\n",
    "        out = self.relu(out)\n",
    "        #out = self.dropout(out)\n",
    "\n",
    "        # 1st fc layer -> 2nd fc layer\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # 2nd fc layer -> 3rd fc layer\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # 3rd fc layer -> output layer\n",
    "        out = self.fc4(out)\n",
    "        #out = self.relu(out)\n",
    "        #out = self.dropout(out)\n",
    "\n",
    "        return (out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model and restoring state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (batch_norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (batch_norm_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (batch_norm_4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (batch_norm_5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=6400, out_features=4096, bias=True)\n",
       "  (batch_norm_6): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "model = SimpleCNN()\n",
    "state_dict = torch.load(\"simpleCNN_2019-11-04 04_44_47.pt\")\n",
    "#print(state_dict)\n",
    "model.load_state_dict(state_dict)\n",
    "#print(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = os.getcwd() + \"\\\\processedscreenshots\"\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)\n",
    "                                        )])\n",
    "# test_data = datasets.ImageFolder(root=dataset, transform=transform)\n",
    "# test_loader  = data.DataLoader(test_data, batch_size=4, shuffle=False, num_workers=2)\n",
    "# correct = 0\n",
    "# total = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    image_tensor = transform(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input = Variable(image_tensor)\n",
    "    input = input.to(device)\n",
    "    output = model(input)\n",
    "    print(output)\n",
    "    index = output.data.cpu().numpy().argmax()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1275,  0.0227]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.6070, -2.2763]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1081, -0.2840]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3167, -3.3436]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.3012, -1.9070]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4965, -3.4874]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2839, -0.5027]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1926, -0.3111]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.7707, -1.0626]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1954, -3.2311]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.3428, -1.9621]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.7461, -2.6303]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.0147, -0.0847]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2022, -0.3838]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.1050, -1.5150]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9732, -2.7907]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.6830, -2.2968]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2040, -0.3365]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.6981, -1.0702]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1821,  0.1183]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.9979, -1.4260]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2530, -0.3185]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.0316, -1.5854]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.4057,  0.2482]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.4041, -0.7034]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.7198, -2.4410]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.5736, -0.7752]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.3180, -0.5826]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.4346, -0.7456]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.4830, -2.1675]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2384,  0.2903]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2865, -0.4343]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.9656, -1.4662]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2690, -0.5143]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.8779, -1.4254]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.3107, -1.8953]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.8898, -2.7216]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2604, -0.4781]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.7799, -1.1666]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.7278, -2.4070]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.1856, -1.7339]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.6442,  0.5858]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.7806, -1.2051]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.5639, -2.4463]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.3936, -0.7345]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.3861, -0.5369]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.8828, -1.3322]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.6635, -1.0889]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.8751, -1.2581]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.3247, -0.4475]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.5272, -1.0318]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.7324, -1.0559]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.4692, -0.8423]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.8925, -2.7316]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.3721, -0.6352]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0034, -2.9081]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.8192,  0.6772]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1668, -3.1240]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.8571, -1.1440]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.9929, -1.3964]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1234, -0.2022]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2153, -0.3389]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.8498, -1.3328]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.0342, -1.7017]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.3071,  0.3459]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.3272, -1.7662]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.0573, -0.2468]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.5490, -2.2316]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.7217, -2.5622]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.7674, -1.2935]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.2622, -1.8150]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.4198, -0.6416]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.4121, -0.6222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.1230,  0.1776]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2967, -0.6607]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1627, -0.3500]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.5452, -2.3214]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.6053, -2.3434]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.8629, -1.4055]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1469, -0.3719]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.2572,  0.4088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2478, -0.4214]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.7817, -1.1871]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.3156, -2.0173]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.2402, -1.8310]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.0018, -1.6319]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1163, -0.2112]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.7219, -2.4919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.4300, -2.0508]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.4212, -0.8720]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.4406, -2.1379]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.3050, -0.5288]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.7490, -1.0376]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1213, -0.3014]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1458, -0.3076]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.7761, -1.1676]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.4419, -0.6915]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.2067, -1.7251]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.7012, -2.4693]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.6538, -1.0113]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.0506,  0.0227]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.8883, -2.6640]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1828, -0.3854]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.4401, -0.5850]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.0733, -1.6273]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2890, -0.4803]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.7620, -2.4939]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.2308, -1.8112]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.7478, -2.5048]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.0111, -1.4640]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3150, -3.3209]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.0477, -1.5678]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.6755, -1.0269]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.1753, -1.8004]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.8699, -1.2886]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4271, -0.7671]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.2648, -2.0356]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.2934, -0.4322]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9259, -2.7886]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.5848, -2.3279]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.7480, -1.0575]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.0765, -0.2286]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "task finished.\n"
     ]
    }
   ],
   "source": [
    "filenames = os.listdir(dataset)\n",
    "# print(filenames)\n",
    "classes = ['neg', 'pos']\n",
    "to_pil = transforms.ToPILImage()\n",
    "model.cuda()\n",
    "output_file = \"output.csv\"\n",
    "file_count = len(filenames)\n",
    "bar = Bar('Processing', max=20)\n",
    "\n",
    "for i in range(file_count):\n",
    "    #print(filenames[i])\n",
    "    image_to_be_predicted = Image.open(dataset+\"\\\\\"+filenames[i])\n",
    "    #image_to_be_predicted = dataset+\"\\\\\"+filenames[i]\n",
    "    img_t = transform(image_to_be_predicted)\n",
    "    img_t = to_pil(img_t)\n",
    "    index = predict_image(img_t)\n",
    "    data = [filenames[i], classes[index]]\n",
    "    #print(index)\n",
    "    #print(\"class: \" + classes[index])\n",
    "    if (i==0):\n",
    "        with open(output_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(data)\n",
    "    else:\n",
    "        with open(output_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(data)\n",
    "    bar.next()\n",
    "\n",
    "bar.finish()\n",
    "print(\"task finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
